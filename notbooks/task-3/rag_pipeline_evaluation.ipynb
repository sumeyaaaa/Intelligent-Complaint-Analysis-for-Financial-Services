{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc862c26",
   "metadata": {},
   "source": [
    "# üöÄ Comprehensive RAG Pipeline Evaluation\n",
    "\n",
    "This notebook is the final tool for evaluating our advanced RAG pipeline.\n",
    "\n",
    "**Workflow:**\n",
    "1.  Loads the high-performance `BAAI/bge-large-en-v1.5` model and re-ranker.\n",
    "2.  Loops through a formal list of evaluation questions from a CSV file.\n",
    "3.  For each question, it generates an answer and displays it with its sources.\n",
    "4.  It then prompts for a **manual quality score (1-5)** and **qualitative comments**.\n",
    "5.  Finally, it saves all results, including the manual scores, to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9437bcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src path added: c:\\Users\\ABC\\Desktop\\10Acadamy\\week_6\\Intelligent-Complaint-Analysis-for-Financial-Services\\src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go two levels up from the notebook to the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "\n",
    "# Join the path to 'src'\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "\n",
    "# Add 'src' to Python path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Confirm it's added\n",
    "print(\"src path added:\", src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2948f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ABC\\Desktop\\10Acadamy\\week_6\\Intelligent-Complaint-Analysis-for-Financial-Services\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import pipeline\n",
    "from typing import Dict, List, Tuple\n",
    "from RAG_pipeline_eval import RAGPipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de6d23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- THE FIX IS HERE ---\n",
    "# 1. Import your prompt template from your module\n",
    "from prompts import PROMPT_TEMPLATE\n",
    "\n",
    "# 2. Update the config to use the imported template\n",
    "config = {\n",
    "    \"embedding\": { \"model_name\": \"BAAI/bge-base-en-v1.5\" },\n",
    "    \"reranker\": { \"model_name\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\", \"k\": 5 },\n",
    "    \"llm\": { \"model_name\": \"google/flan-t5-base\", \"max_new_tokens\": 256 },\n",
    "    \"retrieval\": { \"k\": 25 },\n",
    "    \"prompt\": {\n",
    "        \"template\": PROMPT_TEMPLATE # Use the imported variable here\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"index_path\": \"data/vector_store/index_bge_large_300_20.faiss\",\n",
    "        \"meta_path\": \"data/vector_store/meta_bge_large_300_20.csv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3f51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\ABC\\AppData\\Local\\Temp\\ipykernel_6644\\4092469083.py:5: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  eval_df = pd.read_csv(\"..\\..\\csv_files\\evaluation_dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing RAG Pipeline ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error in __cdecl faiss::FileIOReader::FileIOReader(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:68: Error: 'f' failed: could not open data/vector_store/index_bge_large_300_20.faiss for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_6644\\4092469083.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Initialize the pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m rag_system = RAGPipeline(config=config)\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 2. Load the evaluation questions from your CSV file\u001b[39;00m\n\u001b[32m      5\u001b[39m eval_df = pd.read_csv(\u001b[33m\"..\\..\\csv_files\\evaluation_dataset.csv\"\u001b[39m)\n",
      "\u001b[32mc:\\Users\\ABC\\Desktop\\10Acadamy\\week_6\\Intelligent-Complaint-Analysis-for-Financial-Services\\src\\RAG_pipeline_eval.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m __init__(self, config: Dict):\n\u001b[32m     10\u001b[39m         self.config = config\n\u001b[32m     11\u001b[39m         print(\u001b[33m\"--- Initializing RAG Pipeline ---\"\u001b[39m)\n\u001b[32m     12\u001b[39m         self.embedder = SentenceTransformer(self.config[\u001b[33m'embedding'\u001b[39m][\u001b[33m'model_name'\u001b[39m], device=\u001b[33m'cpu'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         self.index = faiss.read_index(self.config[\u001b[33m'data'\u001b[39m][\u001b[33m'index_path'\u001b[39m])\n\u001b[32m     14\u001b[39m         self.metadata = pd.read_csv(self.config[\u001b[33m'data'\u001b[39m][\u001b[33m'meta_path'\u001b[39m])\n\u001b[32m     15\u001b[39m         self.reranker = CrossEncoder(self.config[\u001b[33m'reranker'\u001b[39m][\u001b[33m'model_name'\u001b[39m])\n\u001b[32m     16\u001b[39m         self.llm = pipeline(\u001b[33m\"text2text-generation\"\u001b[39m, model=self.config[\u001b[33m'llm'\u001b[39m][\u001b[33m'model_name'\u001b[39m])\n",
      "\u001b[32mc:\\Users\\ABC\\Desktop\\10Acadamy\\week_6\\Intelligent-Complaint-Analysis-for-Financial-Services\\venv\\Lib\\site-packages\\faiss\\swigfaiss_avx2.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m  11140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m read_index(*args):\n\u001b[32m> \u001b[39m\u001b[32m11141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _swigfaiss_avx2.read_index(*args)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error in __cdecl faiss::FileIOReader::FileIOReader(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:68: Error: 'f' failed: could not open data/vector_store/index_bge_large_300_20.faiss for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the pipeline\n",
    "rag_system = RAGPipeline(config=config)\n",
    "\n",
    "# 2. Load the evaluation questions from your CSV file\n",
    "# Using a corrected, more standard path\n",
    "eval_df = pd.read_csv(\"../evaluation/evaluation_dataset.csv\")\n",
    "questions = eval_df[\"question\"].tolist()\n",
    "print(f\"\\nLoaded {len(questions)} evaluation questions.\")\n",
    "\n",
    "# 3. Loop through questions, generate answers, and collect manual feedback\n",
    "evaluation_results = []\n",
    "\n",
    "for q in questions:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üîç Evaluating Question: {q}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # UPDATED: Unpack three values (answer, sources, sentiment)\n",
    "    answer, sources, sentiment = rag_system.query(q)\n",
    "    \n",
    "    print(f\"\\nüß† Generated Answer:\\n{answer}\\n\")\n",
    "\n",
    "    # NEW: Display the sentiment summary\n",
    "    print(\"--- Sentiment of Sources ---\")\n",
    "    print(f\"üìä {sentiment}\")\n",
    "    print(\"----------------------------\\n\")\n",
    "\n",
    "    print(\"--- Top Sources Used ---\")\n",
    "    for i, source in enumerate(sources, 1):\n",
    "        print(f\"Source {i}: {source['chunk_text'][:200]}...\")\n",
    "    print(\"-\" * 26)\n",
    "\n",
    "    # Manual input for scoring\n",
    "    while True:\n",
    "        try:\n",
    "            quality_score = int(input(\"üíØ Enter quality score (1‚Äì5): \"))\n",
    "            if 1 <= quality_score <= 5:\n",
    "                break\n",
    "            else:\n",
    "                print(\"‚ùå Please enter a number between 1 and 5.\")\n",
    "        except ValueError:\n",
    "            print(\"‚ùå Invalid input. Please enter a number.\")\n",
    "    \n",
    "    comments = input(\"üìù Enter your comments/analysis: \")\n",
    "\n",
    "    # UPDATED: Add the sentiment summary to the results\n",
    "    evaluation_results.append({\n",
    "        \"Question\": q,\n",
    "        \"Generated Answer\": answer,\n",
    "        \"Sentiment of Sources\": sentiment, # New column\n",
    "        \"Retrieved Sources\": \" | \".join([s['chunk_text'] for s in sources]),\n",
    "        \"Manual Quality Score (1-5)\": quality_score,\n",
    "        \"Comments/Analysis\": comments\n",
    "    })\n",
    "\n",
    "# 4. Save the results to a CSV file\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "results_df.to_csv(\"../evaluation/manual_evaluation_results.csv\", index=False)\n",
    "\n",
    "print(\"\\n\\n‚úÖ Evaluation complete! Results saved to 'evaluation/manual_evaluation_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "\n",
    "print(\"\\n--- Starting Automated RAGAs Evaluation ---\")\n",
    "\n",
    "# 1. Load the manual evaluation results you just created\n",
    "manual_results_df = pd.read_csv(\"../../csv_file/manual_evaluation_results.csv\")\n",
    "\n",
    "# 2. Format the data for RAGAs (RAGAs needs the sources as a list of strings)\n",
    "# Note: We are splitting the saved string of sources back into a list\n",
    "ragas_data = {\n",
    "    \"question\": manual_results_df[\"Question\"].tolist(),\n",
    "    \"answer\": manual_results_df[\"Generated Answer\"].tolist(),\n",
    "    \"contexts\": [s.split(' | ') for s in manual_results_df[\"Retrieved Sources\"]],\n",
    "}\n",
    "ragas_dataset = Dataset.from_dict(ragas_data)\n",
    "\n",
    "# 3. Run the RAGAs evaluation\n",
    "result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    ")\n",
    "\n",
    "# 4. Combine your manual scores with the automated RAGAs scores\n",
    "ragas_df = result.to_pandas()\n",
    "comprehensive_df = pd.concat([manual_results_df, ragas_df.drop(columns=['question', 'answer', 'contexts'])], axis=1)\n",
    "\n",
    "# 5. Save the final comprehensive report\n",
    "comprehensive_df.to_csv(\"evaluation/comprehensive_evaluation_results.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Comprehensive evaluation complete!\")\n",
    "print(\"Final results with both manual and RAGAs scores saved to 'evaluation/comprehensive_evaluation_results.csv'\")\n",
    "\n",
    "display(comprehensive_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
